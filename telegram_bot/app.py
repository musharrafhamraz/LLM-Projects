import os
from typing import Final
from telegram import Update
from phi.agent import Agent
from groq import Groq
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv

load_dotenv()

TELEGRAM_API_KEY = os.getenv("TELEGRAM_API_KEY")
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

USERNAME: Final = "@sab_pata_bot"

async def start_command(update:Update, context:ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Hello! Thanks for the chat.")


async def help_command(update:Update, context:ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("I am SABPataBot, Ask me anything.")


async def custom_command(update:Update, context:ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("This is a custom command.")


# Response

# agent = Agent(
#     model=Groq(id="llama-3.3-70b-versatile"),
#     debug_mode=True  # Enables detailed logs
# )

def handle_response(user_input: str) -> str:
    """
    Takes user input, passes it to the LLM model via the agent, and returns the response.
    
    Args:
        user_input (str): The input text for the agent.
    
    Returns:
        str: The response generated by the agent.
    """
    try:
        print(f"User input: {user_input}")
        # return agent.print_response(user_input) or "Sorry, I didn't understand that."
        system_message = "You are an AI assistant"
        messages = [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_input},
                ]

                # Use the Groq client to generate the response
        completion = client.chat.completions.create(
            model="mixtral-8x7b-32768",  # Adjust the model name if necessary
            messages=messages,
            temperature=0.7,
            max_tokens=1024,
            top_p=0.95,
            stream=False,
                )
        # Get the response content
        output = completion.choices[0].message.content.strip()

                # Post-process the output if necessary
        if output.startswith("Assistant:"):
            output = output[len("Assistant:"):].strip()

        return output 
    except Exception as e:
        print(f"Error during response generation: {e}")
        return f"An error occurred: {e}"




async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    message_type: str = update.message.chat.type
    text: str = update.message.text

    print(f'User ({update.message.chat.id}) in {message_type}: "{text}"')

    if message_type == 'group':
        if USERNAME in text:
            new_text: str = text.replace(USERNAME, '').strip()
            response: str = handle_response(new_text)
        else:
            return
    else:
        response: str = handle_response(text)

    print('Bot:', response)
    await update.message.reply_text(response)


async def error(update: Update, context: ContextTypes.DEFAULT_TYPE):
    print(f'Update {update} caused error {context.error}')


if __name__ == '__main__':
    print('Starting bot...')
    app = Application.builder().token(TELEGRAM_API_KEY).build()

    # Commands
    app.add_handler(CommandHandler('start', start_command))
    app.add_handler(CommandHandler('help', help_command))
    app.add_handler(CommandHandler('custom', custom_command))

    # Messages
    app.add_handler(MessageHandler(filters.TEXT, handle_message))

    # Errors
    app.add_error_handler(error)

    # Polls the bot
    print('Polling...')
    app.run_polling(poll_interval=3)

